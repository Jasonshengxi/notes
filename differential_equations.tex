\documentclass[12pt]{article}
\input{common.tex}

\begin{document}
% Lectured by Christopher Thomas
% Email: c.e.thomas@damtp.cam.ac.uk

\setcounter{section}{-1}
\section{Introduction}

DEs are cool.

For example, Newton's
\[
    m \diff[2]xt = F(x,t)
\]
Where $m$ is mass, $F$ is force, and we call
$x$ the \emph{dependent variable} and $t$ the \emph{independent variable}.

\section{Basic Calculus}

\subsection{Differentiation}

\begin{definition}\label{def:derivative}

    The derivative of a fn $f(x)$ wrt its argument $x$ is
    the function
    \[
        \diff f x = \lim_{h\to 0}\frac{f(x+h) - f(x)}{h}.
    \]
    For the derivative to exist, we require the left-handed
    and right-handed limits to both exist and be equal.
\end{definition}

For example, $|x|$ is not differentiable at $0$ since
the right limit is $1$ and the left limit is $-1$.
(it is however differentiable everywhere else)

\subsubsection*{Order parameters}

Consider the behavior of a function close to a limiting point, $x_{0}$.

\subsubsection*{Big O}

\emph{Big O} roughly means \emph{can be bounded by}.

\begin{definition}
    For finite $x_{0}$, we say
    $f(x)$ is $O(g(x))$ as $x \to x_{0}$ if
    there exists $\delta >0$ and $M > 0$ s.t.
    for all $x$ with $0 < |x - x_{0}| < \delta $,
    we have $|f(x)| \le M|g(x)|$.
\end{definition}

It follows that $\dfrac{f(x)}{g(x)}$ is bounded as $x \to x_{0}$.

Note that this definition is for $x_{0}$ finite,
whereas most usages of $O$, for instance for time complexity,
uses it as $x_{0} = \infty$.

Examples:
\begin{compactitem}
    \item $x \ne O(x^{2})$ as $x \to 0$.
    \item $x^{2} = O(x)$ as $x \to 0$.
    \item $x = O(\sqrt{x})$ as $x \to 0$.
    \item $\sin(2x) = O(x)$ as $x \to 0$, since $|\sin 2x| \le 2|x|$ for all $x$.
\end{compactitem}

\begin{definition}
    For infinite $x_{0}$, we say
    $f(x)$ is $O(g(x))$ as $x \to \infty$
    if there exists $x_{1}$ and $M > 0$ s.t.
    for all $x > x_{1}$, we have
    $|f(x)| \le M|g(x)|$.
\end{definition}

For example, $2x^{3}+4x+12 = O(x^{3})$ as $x \to \infty$.

\subsubsection*{Little o}
\emph{Little o} roughly means \emph{much smaller than}.
It is written $\underline{o}$ in handwritten math, 
to distinguish from $O$.

\begin{definition}
    For finite $x_{0}$, we say
    $f(x)$ is $o(g(x))$ as $x \to x_{0}$ if,
    for any $\varepsilon > 0$, there exists $\delta >0$ s.t.
    for all $0 < |x - x_{0}| < \delta $,
    we have $|f(x)| \le \varepsilon |g(x)|$.
\end{definition}

Note that if $g(x) \ne 0$ in the vicinity of $x_{0}$,
but not necessarily at $x_{0}$, the above definition is equivalent to
\[
\lim\limits_{x\to x_{0}} \frac{f(x)}{g(x)} = 0.
\]
For example, $x^{2} = o(x)$ as $x \to 0$, since $\lim\limits_{x \to 0} \frac{x^{2}}{x} = 0$.

The infinite definition is similar.

\subsubsection*{Notes}

\textbf{$f(x) = o(g(x))$ is a stronger statement than $f(x) = O(g(x))$}.
Intuitively, big $O$ means bounded by some given multiple,
whereas little $o$ means bounded by any multiple.

Thus, $f(x) = o(g(x))$ implies $f(x) = O(g(x))$,
but the converse does not hold.

\textbf{Constants \emph{don't} matter}. 
If $f(x) = O(g(x))$, then $af(x) = O(g(x))$ and $f(x) = O(ag(x))$
for any $a \ne 0$.

\subsubsection*{Usage}

Order parameters are useful to classify remainder terms before taking limits.
Let $\varepsilon (h)$ in
\[
f(x_{0}+h) - f(x_{0}) = hf'(x_0) + \varepsilon (h),
\]
for some finite $h$. Then we have,
by dividing by $h$ on both sides and taking the limit,
\[
\lim\limits_{h \to 0}\left[\frac{f(x_{0}+h)-f(x_{0})}{h}\right]
= f'(x_{0}) + \lim\limits_{h \to 0}\left[\frac{\varepsilon (h)}{h}\right]
\]
The limit on the RHS vanishes by the definition of $f'$.
Therefore, $\varepsilon (h) = o(h)$ as $h \to 0$.

\newpage
\subsection{Rules for Differentiation}

\begin{theorem}[Chain Rule]
    Given $f(x) = F(g(x))$, we have
    \[
    \diff f x = F'(g(x)) \diff gx = \diff Fg \diff gx,
    \]
    where $F'$ denotes the derivative of $F$ wrt to its argument,
    and $F'(g(x))$ denotes the evaluation of that derivative at $g(x)$.
\end{theorem}

\begin{theorem}[Product Rule]
    Given $f(x) = u(x)v(x)$, we have
    \[
    \diff f x = v \diff ux + u \diff vx.
    \]
\end{theorem}

\begin{theorem}[Leibniz's Rule]
    Given $f(x) = u(x)v(x)$, we have
    \[
    f^{(n)} = \sum\limits_{r=0}^{n}\binom nr \left(u^{(r)} \cdot v^{(n-r)}\right).
    \]
\end{theorem}

\appendix

\newpage
\section{Notation}

\subsection{Limits}

For left and right limits, we may write $\lim\limits_{h \to 0^{-}}$ and $\lim\limits_{h \to 0^{+}}.$ 

\subsection{Derivatives}

For derivatives, we may write
\[
    \diff f x = f'(x) = \dot f(x).
\]
Similarly, we have, for a second derivative,
\[
    \diff {}x \diff f x = \diff[2] f x = f''(x) = \ddot f(x).
\]
And for an arbitrary $n$th derivative, we may write
\[
    \diff[n] f x = f^{(n)}(x).
\]

\subsection{Order parameters}

Some may write $f(x) = O(g(x))$ when $f(x)$ is $O(g(x))$.
Some say it is better to write $f(x) \sim O(g(x))$.
It is up to the reader to decide which one is better.

\end{document}
