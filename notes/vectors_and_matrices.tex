\documentclass[12pt]{article}
\input{../common.tex}

\begin{document}
% Lectured by Angela Capel Gueras (ac2722)
% Office: BO.14, Department of Applied Mathematics and Theoretical Physics

\section{Complex Numbers}

\subsection{Definition}

We construct $\mathbb{C}$ by adding $i$ to $\mathbb{R}$, where $i^{2} = -1$.
Then, any $z \in \mathbb{C}$ has the form $z = x + iy$, where $x,y \in \mathbb{R}$.
We define the real part, $x = \Re(z)$, and the imaginary part, $y = \Im(z)$.

\subsection{Properties}

\begin{compactenum}[i)]
\item Addition: $z_{1} \pm z_{2} = (x_{1} \pm x_{2}) + i(y_{1} \pm y_{2})$.
\item Multiplication: $z_{1}z_{2} = (x_{1}y_{1} - x_{2}y_{2}) + i(x_{1}y_{2} + x_{2}y_{1})$.
\end{compactenum}
Note that both addition and multiplication are associative and commutative.
\begin{compactenum}[i)]
\setcounter{enumi}{2}
\item Identity: The group $(\mathbb{C},+)$ is an abelian group with identity $0$.
\item Inverse: The \emph{inverse of $z$} is given by
    \[
    z^{-1} = \frac{x - iy}{x^{2} + y^{2}}
    \]
    and it satisfies $z \cdot z^{-1} = 1$.
\end{compactenum}

Thus, $(\mathbb{C}^{*}, \cdot)$ is an abelian group with identity $1$,
where $\mathbb{C}^{*} = \mathbb{C} - \{0\}$.

Note that the distributive property is also satisifed, where
\[
    (z_{1}+z_{2})z_{3} = z_{1}z_{2} + z_{2}z_{3}
\]
\begin{compactenum}[i)]
\setcounter{enumi}{4}
\item Complex Conjugate: For any $z = x + iy$, 
    the \emph{complex conjugate of $z$}, denoted $\bar z$, or $z^{*}$, is equal to $x - iy$.
\end{compactenum}
Thus, we also have $\Re(z) = (z + \bar z) / 2$, and $\Im(z) = (z - \bar z) / 2i$.
We have rules
\begin{compactitem}
\item $\bar {\bar z} = z$
\item $\bar{z_{1} + z_{2}} = \bar z_{1} + \bar z_{2}$
\item $\bar {z_{1}z_{2}} = \bar z_{1} \bar z_{2}$
\end{compactitem}

\begin{compactenum}[i)]
\setcounter{enumi}{5}
\item Modulus: For any $z = x + iy$, we define the \emph{modulus of $z$} by $|z|$, or $r$, by
    a real and non-negative number such that
    \[
    |z|^{2} = x^{2} + y^{2}
    \]
\item Argument: The \emph{argument} of a complex number $z = x + iy \ne 0$
    is a real number, denoted by $\theta = \arg(z)$, such that
    \[
        z = r(\cos \theta + i \sin \theta)
    \]
    Which is the \emph{polar form} of $z$. We can verify
    that $\tan\theta = y/x$.
\end{compactenum}

Note that if $\theta$ is an argument of $z$, so is $\theta + 2\pi n$ for $n \in \mathbb{Z}$.
To make it unique, we restrict $-\pi < \theta \le \pi$.
This value of $\theta$ is the \emph{principal value}.

Remarks:
\begin{compactenum}[(1)]
\item $\mathbb{R} \subset \mathbb{C}$, since for $a \in \mathbb{R}$ we have
    $a = a + i 0 \in \mathbb{C}$.
\item A complex number $0 + ib$ is said to be a \emph{pure imaginary number}.
\item The representation of a complex number in terms
    of its real and imaginary parts is unique.
\end{compactenum}

\subsubsection*{More Properties / Consequences}
\begin{compactenum}[(i)]
\setlength{\parskip}{4pt}
\item $(\mathbb{C},+,\cdot)$ is a field.
\item \emph{Fundamental Theorem of Algebra}:

    A polynomial of degree $n$ with coefficients in $\mathbb{C}$
    can be written as the product of $n$ linear factors:

    \[
        \begin{aligned}
            p(z) &= c_n z^{n} + \cdots + c_{0} \\
                &= c_n (z - \alpha _1)\cdots(z - \alpha_2)
        \end{aligned}
    \]
    
    where $c_i \in \mathbb{C}$, and $c_n \ne 0$, with roots $\alpha_i \in \mathbb{C}$.
    Thus, $p(z) = 0$ has at least one root,
    and $n$ roots counted with multiplicity.
\item The modulus satisifes the following properties:
    \begin{itemize}
        \item $|z_{1}z_{2}| = |z_{1}||z_{2}|$
        \item $|z_{1}| + |z_{2}| \le |z_{1}| + |z_{2}|$
        \item $|z_{1} - z_{2}| \ge \left||z_{1}| - |z_{2}|\right|$
    \end{itemize}
\end{compactenum}

\subsubsection*{De Moivre's Theorem}

\begin{lemma}
    If $z_{1} = r_{1}(\cos \theta_{1} + i \sin \theta_{1})$,
    and $z_{2} = r_{2}(\cos \theta_{2} + i \sin \theta_{2})$,
    then $z_{1}z_{2} = r_{1}r_{2}(\cos (\theta_{1} + \theta_{2}) + i \sin (\theta_{1} + \theta_{2})$.
\end{lemma}

\begin{proof}
    Multiplying naively gives
    \[
    z_{1}z_{2} = r_{1}r_{2}(
        (\cos\theta_{1} \cos\theta_{2} - \sin\theta_{1}\sin\theta_{2})
        +i(\cos\theta_{1} \sin\theta_{2} + \cos\theta_{2} \sin\theta_{1})
    )
    \]
    Applying the addition rule of $\sin$ and $\cos$, the result is obtained.
\end{proof}

\begin{theorem}[De Moivre's]
    \label{thm:de_moivre}
    For any $\theta \in \mathbb{R}$, $n \in \mathbb{Z}$, we have
    \[
        (\cos\theta + i\sin\theta )^{n} = \cos(n\theta ) + i\sin(n\theta ).
    \]
\end{theorem}
\begin{proof}
    The proof is trivial and left as an exercise to the reader.
\end{proof}

\setcounter{subsection}{3}
\subsection{Exponential and Trigonometric Functions}

\subsubsection{Exponential Functions}

We define the exponential function on $z \in \mathbb{C}$ by
\[
    \exp(z) = e^{z} = \sum\limits_{n=0}^{\infty}\frac{1}{n!}z^{n}.
\]
The function only exists when the series converges,
and it does for all $z \in \mathbb{C}$.
A proof will not be provided for that result.

Properties:
\begin{compactitem}
\item $e^{z}e^{w} = e^{z+w}$, for all $z,w \in \mathbb{C}$.
\item If $z \in \mathbb{R}$, then $\exp(z)$ reduces to usual exponential.
\item $e^{0} = 1$.
\item $(e^{z})^{n} = e^{nz}$.
\end{compactitem}

\subsubsection{Trigonometric Functions}

We have
\begin{align*}
    \cos(z) &= \frac{1}{2}(e^{iz} + e^{-iz}) \\
            &= \frac{1}{2}\left(
            \sum\limits_{n=0}^{\infty}\frac{1}{n!}(iz)^{n}
            +\sum\limits_{n=0}^{\infty}\frac{1}{n!}(-iz)^{n} \right) \\
            &= \sum\limits_{n=0}^{\infty}(-1)^{n}\frac{z^{2n}}{(2n)!}
\end{align*}

Similarly, 
\begin{align*}
    \sin(z) &= \frac{1}{2i}(e^{iz} - e^{-iz}) \\
            &= \frac{1}{2i}\left(
            \sum\limits_{n=0}^{\infty}\frac{1}{n!}(iz)^{n}
            -\sum\limits_{n=0}^{\infty}\frac{1}{n!}(-iz)^{n} \right) \\
            &= \sum\limits_{n=0}^{\infty}(-1)^{n}\frac{z^{2n+1}}{(2n+1)!}
\end{align*}

If $z \in \mathbb{R}$, this clearly reduces to the usual trigonometric functions.

It is also possible to notice that
\[
e^{iz} = \cos z + i\sin z,
\]
in particular, if $z = \pi$, then
\begin{equation*}
    e^{i\pi} = -1
\end{equation*}
which is \emph{Euler's Identity}.

\begin{corollary}
    $e^{z} = 1 \iff z = 2\pi ni$,
    where $n \in \mathbb{Z}$.
\end{corollary}
\begin{proof}
    Let $z = x+iy$, then 
    $e^{z} = e^{x}e^{iy} = e^{x}(\cos y+i\sin y) 
    = e^{x}\cos y + ie^{x} \sin y = 1$.
    Then, $e^{x}\sin y = 0$, so $\sin y = 0$, so $y = n\pi$. 
    Similarly, $e^{x}\cos y = 1$,
    substituting gives $e^{x}(\pm 1) = 1$, 
    and since $e^{x}$ is positive, $\cos y = 1$, 
    so $y = 2n\pi$.
    Thus, $e^{x} = 1$, so $x = 0$, and $z = 2\pi n i$.
\end{proof}

Finally, we can write $z = r(\cos\theta + i\sin\theta) = re^{i\theta }$,

\subsubsection{Roots of Unity}

Let $z = re^{i\theta }$, and let for some $N \in \mathbb{N}$ that $z^{N} = 1$.
Then we have
\[
z^{N} = r^{N}e^{i\theta N}.
\]
Trivially then we have $r = 1$, $\theta N= 2\pi n$, so
\[
    z = e^{\frac{2\pi i}{N}n}
\]

\subsection{Logarithm and Complex Powers}

We say, if $z \in \mathbb{C}$ and $z \ne 0$, that $w = \log z$ iff $e^{w} = z$.
By definition, $\log$ is the inverse of $\exp$. 
Note that $\log$ here means the natural logarithm.

Importantly, this is a many-to-one relationship.
\[
r = re^{i\theta } = e^{\log r}e^{i\theta } = e^{\log r + i\theta },
\]
so
\[
\log z = \log r + i\theta  = \log r + i \arg(z)
\]
for any $\theta  \in \Arg(z)$.
Thus, if $r + i\theta $ is a log of $z$, then so is $r + i(\theta + 2\pi)$.
To make it unique, we simply use the principal value of $\arg$,
as in $-\pi < \theta  \le \pi$. 
In contexts where we use all values, we write $\Log(z)$.

\subsubsection*{Complex Powers}

We define $z$ to the power of $\alpha $ to be
\begin{align*}
    z^{\alpha } &= e^{\alpha \Log z}\\
                &= e^{\alpha(\log r + i\theta + i 2n\pi)}
\end{align*}
This is multivalued due to the multi-valued nature of $\Log$.

Examples:
\begin{compactitem}
\item $\log(i) = \log(e^{i\pi / 2}) = i \left(\frac{\pi}{2} + 2\pi n\right)$.
\item $\log(1 + i) = \log\sqrt{2} + i\left(\frac{\pi}{4} + 2\pi n\right)$.
\item $(1 + i)^{1/2} = \sqrt[4]{2} + i\left(\frac{\pi}{8} + \pi n\right)$.
\end{compactitem}

Note that $(e^{a})^{b}$ is not necessarily equal to $e^{ab}$.

\subsection{Lines and Circles}

\subsubsection*{Lines}

Lines are defined by a point $z_{0} \in \mathbb{C}$ and a direction $\omega \in \mathbb{C}$,
and given by
\[
z = z_{0} + \lambda \omega
\]
where $\lambda \in \mathbb{R}$.
Taking conjugates, we have $\bar z = \bar z_{0} + \lambda \bar \omega$,
so $\bar \omega z - \omega \bar z = \bar \omega z_{0} - \omega \bar z_{0}$.

\subsubsection*{Circles}

Circles are defined by a center $c \in \mathbb{C}$ and a radius $\rho > 0$,
and given by
\[
z = c + \rho e^{i\theta}
\]
where $\theta \in \mathbb{R}$. Note that $|z - c| = \rho$.

\section{Vectors}

A vector is specified by a positive magnitude and a direction in a space.
We can represent a vector as a directed line segment between two points $A$ and $B$,
denoted $\vec v = \overrightarrow{AB}$.
Of we choose $O$ as origin, then point $A$ has \emph{position vector}
$\vec a = \overrightarrow{OA}$.

\subsection{Vector Spaces}

\begin{definition}[Vector Space]
    A vector space over a field $F$ is a set $V$
    together with two binary operations:
    vector addition $+ : V \times V \to V$,
    and scalar multiplication $\times : F \times V \to V$,
    satisfying the following axioms:
    \begin{compactitem}
    \item $V$ forms an abelian group under vector addition.
        we write $\vec{0}$ for the identity in this group.
    \item Compatibility of vector addition and scalar multiplication:
        $a(b\vec{v}) = (ab)\vec{v}$.
    \item Write $1$ for the multiplicative identity in $F$, we need $1\vec{v} = \vec{v}$.
    \item Scalar multiplication distributes over vector addition: $a(\vec{u} + \vec{v}) = a\vec{u} + a\vec{v}$.
    \item and vice versa: $(a + b)\vec{v} = a\vec{v} + b\vec{v}$.
    \end{compactitem}
\end{definition}

If $F = \mathbb{R}$, the vector space is \emph{real}.

% \begin{definition}[Vector Space]
%     A vector space over $\mathbb{R}$ or $\mathbb{C}$
%     is a collection of vectors $\vec v \in V$,
%     together with two operations:
%     addition of two vectors, and multiplication by a scalar (from $\mathbb{R}$ or $\mathbb{C}$, respectively).
% \end{definition}

% To add vectors, join vectors end-to-end.

\begin{center}
    \begin{tikzpicture}[scale=1.5]
        \draw (0,0) coordinate (O) node[below left] {O};
        \draw[->] (O) -- node[pos=0.5, below] {$\vec a$} (0:3) coordinate (A) node[below right] {A};
        \draw[->] (O) -- node[pos=0.5, above left] {$\vec b$} (60:2) coordinate (B) node[above left] {B};
        \draw[dashed] (A) -- ++(B) coordinate (C) node [above right] {C};
        \draw[dashed] (B) -- ++(A);
        \draw[->] (O) -- node[pos=0.5, above] {$\vec c$} (C);
    \end{tikzpicture}
\end{center}

% Note that this definition is close, commutative, associative, 
% has an identity and has an inverse.
% Thus, $(V,+)$ forms an abelian group.

% Scalar multiplication distributes over vector addition, over scalar addition,
% is associative with scalar multiplication, and has an identity of $1$.

\begin{definition}
    A \emph{unit vector} has length $1$.
    We may denote it $\unitvec v$.
\end{definition}

\begin{definition}
    Given vectors $\vec a, \vec b$ and scalars $\alpha ,\beta \in \mathbb{R}$,
    then $\alpha \vec a + \beta \vec b$ is a \emph{linear combination} of $\vec a$ and $\vec b$.
\end{definition}

This definition trivially applies to linear combinations of more than two vectors.

\begin{definition}
    The \emph{span} of a collection of vectors
    is the set formed by their linear combinations,
    $\vecspan\{\vec a, \vec b\}=\{\alpha \vec a + \beta \vec b : \alpha,\beta \in \mathbb{R}\}$
\end{definition}

We say $\vec{a}$ and $\vec{b}$ are parallel,
denoted $\vec{a}\parallel\vec{b}$, if there exists non-zero scalar $\lambda $
such that $a = \lambda b$.
We write $\vec a \nparallel \vec b$ otherwise.
If $\vec a \nparallel \vec b$, then $\vecspan\{\vec a, \vec b\}$ is a plane.

Under these definitions, $\mathbb{R}^{n}$ is a vector space,
where addition is addition by component, and scalar multiplication.

\subsection{Dot Product}

The dot / scalar / inner product,
$\vec{a}\cdot\vec{b}$,
is a binary operation from the vectors
to the scalar space ($\mathbb{R}$ or $\mathbb{C}$).

It is "defined by" $\vec{a}\cdot\vec{b} = |\vec{a}||\vec{b}|\cos\theta$.
If either $\vec{a}$ or $\vec{b}$ is zero, we define $\vec{a}\cdot\vec{b}=0$.

Properties:
\begin{compactitem}
\item The dot product has $\vec{a}\cdot\vec{a}=|\vec{a}|^{2}\ge 0$,
    with equality iff $\vec{a}=\vec{0}$,
\item Associative with scalar multiplication, as in
    $(\lambda \vec{a})\cdot\vec{b}=\lambda (\vec{a}\cdot\vec{b}) = \vec{a}\cdot(\lambda \vec{b})$,
\item Commutative,
\item Distributive over vector addition.
\end{compactitem}

We say that $\vec{a}$ and $\vec{b}$ are \emph{orthogonal},
denoted $\vec{a}\perp\vec{b}$, iff $\vec{a}\cdot\vec{b}=0$.
Note that $\vec{0}$ is thus orthogonal to every vector.

\begin{definition}
    In a real vector space $\mathcal{V}$,
    an inner product is a map $\cdot : V \times V \to \mathbb{R}$
    that satisfies:
    \begin{compactenum}[(i)]
    \item Commutativity: $\vec{a}\cdot\vec{b} = \vec{b}\cdot\vec{a}$.
    \item Linearity in the second argument: $\vec{x}(\lambda \vec{y}+\mu \vec{z}) = \lambda \vec{x}\cdot\vec{y}+\mu \vec{x}\cdot\vec{z}$.
    \item Positive definiteness: $\vec{x}\cdot\vec{x} \ge 0$ with equality iff $\vec{x}=\vec{0}$.
    \end{compactenum}
    It is denoted $\vec{x}\cdot\vec{y}$ or $\langle\vec{x}|\vec{y}\rangle$.
\end{definition}

\begin{definition}
    In a real vector space with an inner product,
    the norm of a vector $\vec{a}$ is $|\vec{a}| = \sqrt{\vec{a}\cdot\vec{a}}$.
\end{definition}

\begin{theorem}[Cauchy-Schwarz Inequality]
    For all $\vec{x},\vec{y} \in \mathbb{R}^{n}$, we have $|\vec{x}\cdot \vec{y}| \le |\vec{x}||\vec{y}|$.
    \begin{proof}
        Consider the expression $|\vec{x} - \lambda \vec{y}|^{2}$, where $\lambda \in \mathbb{R}$.
        Then, mlearly, we have
        \begin{align*}
            |\vec{x} - \lambda \vec{y}|^{2} &\ge 0 \\
            (\vec{x} - \lambda \vec{y})\cdot(\vec{x} - \lambda \vec{y}) &\ge 0\\
            |\vec{x}|^{2} + \lambda^{2}|\vec{y}|^{2} - 2\lambda \vec{x}\cdot\vec{y} &\ge 0\\
            (|\vec{y}|^{2})\lambda ^{2} + (-2\vec{x}\cdot\vec{y})\lambda + |\vec{x}|^{2} & \ge 0\\
        \end{align*}
        Hence we have a quadratic inequality in $\lambda $, with a positive first coefficient.
        Solving for the roots we have
        \[
        \lambda = \frac{2\vec{x}\cdot\vec{y} \pm \sqrt{4(\vec{x}\cdot\vec{y})^{2} - 
        4|\vec{x}|^{2}|\vec{y}|^{2}}}
        {2|\vec{y}|^{2}}.
        \]
        Since the equality holds for all $\vec{x},\vec{y}$, there can
        never be two roots, so by consider the discriminant we have
        \[
        4(\vec{x}\cdot\vec{y})^{2} - 4|\vec{x}|^{2}|\vec{y}|^{2} \le 0.
        \]
        Trivially rearranging we have
        \[
        |\vec{x}\cdot\vec{y}|^{2} \le \left(|\vec{x}||\vec{y}|\right)^{2},
        \]
        so taking square roots on both sides we have $|\vec{x}\cdot\vec{y}| \le |\vec{x}||\vec{y}|$.
    \end{proof}
\end{theorem}
Thus, the inequality holds for all possible scalar products
on any real vector space.

Note that equality holds iff $\vec{x} = \lambda \vec{y}$ or 
$\vec{y} = \lambda \vec{x}$ for some $\lambda  \in \mathbb{R}$.

We can now define $\theta $ by $\vec{x}\cdot\vec{y} = |\vec{x}||\vec{y}|\cos\theta $,
since Cauchy-Schwarz guarantees that $-1 \le \cos\theta \le 1$.
Note that this technically has multivalued $\theta $,
which shouldn't matter since $\theta $ only interacts through trig.

\begin{theorem}[Triangle Inequality]
    $|\vec{x} + \vec{y}| \le |\vec{x}| + |\vec{y}|$
\end{theorem}
\begin{proof}
    \begin{align*}
        |\vec{x}+\vec{y}|^{2} &= (\vec{x} + \vec{y})\cdot(\vec{x} + \vec{y})\\
                              &= |\vec{x}|^{2} + |\vec{y}|^{2} + 2\vec{x}\cdot\vec{y}\\
                              &\le |\vec{x}|^{2}+|\vec{y}|^{2}+2|\vec{x}||\vec{y}|\\
                              &= (|\vec{x}| + |\vec{y}|)^{2}
    \end{align*}
    Thus, $|\vec{x}+\vec{y}| \le |\vec{x}| + |\vec{y}|$.
\end{proof}

\subsection{Orthonormal Bases}

\begin{definition}
    For any set of vectors $\{\vec{e}_i\}$, they are orthonormal iff
    we have $e_i \cdot e_j = \delta _{ij}$. 
\end{definition}
This immediately implies $|e_i| = 1$ and $e_i \cdot e_j = 0$ if $i \ne j$,
i.e. every vector is normalized and they are pairwise orthogonal.

\begin{definition}
    A set of vectors $\{e_i\}$ is a basis of a vector space
    if every vector can be written as a unique linear combination of $\{e_i\}$.
    A basis that is orthonormal is an orthonormal basis.
\end{definition}

Consider $\mathbb{R}^{3}$, and consider vectors $\vec{e}_1, \vec{e}_2, \vec{e}_3$ 
orthonormal to each other. 
This is equivalent to a cartesian axes along these directions.

This forms a basis of $\mathbb{R}^{3}$, specifically an orthonormal basis.

By definition, for any $\vec{a} \in \mathbb{R}^{3}$, we have $\vec{a} = a_i \vec{e}_i$.
We will use row vector and 
column vector notation to write $\vec{a}$ using its components.

Using the inner product in $\mathbb{R}^{3}$, we have
\begin{align*}
    \vec{a}\cdot\vec{b} &= (a_i \vec{e}_i) \cdot (b_j \vec{e}_j)\\
                    &= a_i b_j (\vec{e}_i \cdot \vec{e}_j)\\
                    &= a_i b_j \delta _{ij}\\
                    &= a_i b_i
\end{align*}
In particular, we have $\vec{a}\cdot\vec{a} = |\vec{a}|^{2} = a_i a_i$, 
which is the Pythagorean theorem.

Specifically in $\mathbb{R}^{3}$, we define the 
canonical basis by have any orthonormal basis,
and denoting them $\vec{i}, \vec{j}, \vec{k}$. By definition,
$\vec{i} = (1,0,0)$, $\vec{j} = (0,1,0)$, and $\vec{k} = (0,0,1)$.

\subsection{Vector Product}

The vector product is a product between vectors,
only on $\mathbb{R}^{3}$.

It is defined geometrically by 
$\vec{a}\times\vec{b} = |\vec{a}||\vec{b}|\sin(\theta) \unitvec{n}$,
where $\unitvec{n}$ is a unit vector perpendicular to both $\vec{a}$ and $\vec{b}$.
Note that two such vectors exist, and we will use the right-handed convention.
We may also write $\vec{a}\wedge\vec{b}$ for $\vec{a}\times\vec{b}$.

When $\vec{a}$ and $\vec{b}$ are parallel,
$\unitvec{n}$ is not defined, but
$\sin\theta = 0$, so we define $\vec{a}\times\vec{b} = \vec{0}$.
If $\vec{a}$ or $\vec{b}$ are zero,
then $\theta $ is not defined, but $|\vec{a}|$ or $|\vec{b}|$ is zero,
so we define $\vec{a}\times\vec{b} = \vec{0}$.

Properties:
\begin{compactitem}
\item $\vec{a}\times\vec{b} = -\vec{b}\times\vec{a}$.
\item $\vec{a}\times\vec{a} = \vec{0}$.
\item the only cases when $\vec{a}\times\vec{b} = \vec{0}$ are the special cases.
\item $(\lambda \vec{a})\times\vec{b} = \lambda (\vec{a}\times\vec{b}) = \vec{a}\times(\lambda \vec{b})$.
\item $\vec{a}\times(\vec{b}+\vec{c}) = \vec{a}\times\vec{b} + \vec{a}\times\vec{c}$.
\item $\vec{a}\cdot(\vec{a}\times\vec{b}) = \vec{b}\cdot(\vec{a}\times\vec{b}) = 0$.
\end{compactitem}

Geometrically, the product $\vec{a}\times\vec{b}$ is the
vector area of the parallelogram defined by 
$\vec{0}$, $\vec{a}$, $\vec{a}+\vec{b}$, and $\vec{b}$,
since $|\vec{a}\times\vec{b}| = |\vec{a}||\vec{b}|\sin\theta $.

Note that in the special case of $\vec{a}$ and $\vec{x}$ such that
$\vec{a} \perp \vec{x}$, $\vec{a} \times \vec{x}$
yields a vector that has length $|\vec{x}||\vec{a}|$ and
is the rotation of $\vec{x}$ by $\pi/2$ in the plane 
defined by $\vec{a}$.

We have that $\vec{e}_i \times \vec{e}_j = x\vec{e}_k$ where $x = \epsilon_{ijk}$.
Note that suffix notation here is not being used for summation.
Under these definitions and the fact that
cross product is linear, we can show that
\begin{align*}
    \vec{a} \times \vec{b} &= (a_{2}b_{3} - a_{3}b_{2})\vec{e}_1 \\
                           &+ (a_{3}b_{1} - a_{1}b_{3})\vec{e}_2 \\
                           &+ (a_{1}b_{2} - a_{2}b_{1})\vec{e}_3
\end{align*}
which is the algebraic definition of cross product in $\mathbb{R}^{3}$.
An equivalent formulation is
\[
\vec{a} \times \vec{b} = \left|\begin{matrix}
    \vec{i} & \vec{j} & \vec{k}\\
    a_{1} & a_{2} & a_{3}\\
    b_{1} & b_{2} & b_{3}
\end{matrix}\right|
\]
\subsection{Triple Products}

\subsubsection{Scalar Triple Product}

Define the scalar triple product by 
$[\vec{a},\vec{b},\vec{c}] = \vec{a} \cdot (\vec{b} \times \vec{c})$.
Note that because of the use of the cross product, 
this only applies in $\mathbb{R}^{3}$.
The value of the scalar triple product is invariant under
a cyclic permutation and negates under an anticyclic permutation.

Note that this product can be interpreted
as the signed volume of parallelepiped formed by the three vectors
$\vec{a},\vec{b},\vec{c}$.
The sign is determined by the handedness of the vector set.
By convention, if $\vec{a},\vec{b},\vec{c}$ are right-handed,
we have $[\vec{a},\vec{b},\vec{c}] > 0$.

Also, $[\vec{a},\vec{b},\vec{c}] = 0$ iff $\vec{a},\vec{b},\vec{c}$
are coplanar.

\subsubsection{Vector Triple Product}

We define the vector triple product to be $\vec{a}\times(\vec{b}\times\vec{c})$.
Note that since the cross product is not associative, the brackets matter.

We have that 
$\vec{a} \times (\vec{b} \times \vec{c}) = 
(\vec{a} \cdot \vec{c})\vec{b} - (\vec{a} \cdot \vec{b})\vec{c}$.

\subsection{Lines, Planes, and Vector Equations}

\subsubsection{Lines}

Any point on a line through $\vec{a}$ with direction $\vec{u} \ne 0$ has position vector
\[
    \vec{r} = \vec{a} + \lambda \vec{u}
\]
where $\lambda \in \mathbb{R}$.
Without parameters, we can also write
\[
\vec{u} \times (\vec{r} - \vec{a}) = \vec{0}
\]
or
\[
\vec{u} \times \vec{r} = \vec{u} \times \vec{a}
\]

\subsubsection{Planes}

Any point on a plane through $\vec{a}$
defined by directions $\vec{u}$ and $\vec{v}$ has position vector
\[
\vec{r} = \vec{a} + \lambda \vec{u} + \mu \vec{v}
\]
where $\lambda ,\mu \in \mathbb{R}$. In this case, we can also write
\[
\vec{r} \cdot (\vec{u} \times \vec{v}) = \vec{a} \cdot (\vec{u} \times \vec{v})
\]

\subsubsection*{Shortest Distance Between Two Lines}
Consider two lines
\begin{align*}
    l_{1} : \vec{u}_1 \times (\vec{r} - \vec{a}_1) &= \vec{0}\\
    l_{2} : \vec{u}_2 \times (\vec{r} - \vec{a}_2) &= \vec{0}
\end{align*}
The shortest distance between $l_{1}$ and $l_{2}$ is
obtained at a line perpendicular to both of them.
Thus, it must be parallel to $l_{1} \times l_{2}$.

The shortest distance is then computed by projecting
$\vec{a}_1 - \vec{a}_2$ onto $\vec{u}_1 \times \vec{u}_2$, namely
\[
s = \left|(\vec{a}_1 - \vec{a}_2) \cdot \frac{\vec{u}_1 \times \vec{u}_2}{|\vec{u}_1 \times \vec{u}_2|}\right|
\]
\subsubsection{Vector Equations}

If we wish to solve equations of the form
\begin{equation}
\vec{r} + \vec{a} \times (\vec{b} \times \vec{r}) = \vec{c}
\end{equation}
for $\vec{r}$ given vectors $\vec{a},\vec{b},\vec{c}$.
We can apply the identity $\vec{a} \times (\vec{b} \times \vec{r}) 
= (\vec{a} \cdot \vec{r}) \vec{b} - (\vec{a} \cdot \vec{b})\vec{r}$ to (1), yielding
\begin{equation}
\vec{r} + (\vec{a} \cdot \vec{r}) \vec{b} - (\vec{a} \cdot \vec{b}) \vec{r} = \vec{c}.
\end{equation}
Taking the product of (2) with $\vec{a}$,
\[
\vec{a} \cdot \vec{r} + (\vec{a} \cdot \vec{r})(\vec{a} \cdot \vec{b}) 
- (\vec{a} \cdot \vec{b})(\vec{a} \cdot \vec{r}) = \vec{a} \cdot \vec{c}.
\]
noticing the duplicate term we have
\[
\vec{a}\cdot\vec{r} = \vec{a}\cdot\vec{c}
\]
so we can substitute this back into (2) to yield
\begin{align*}
    \vec{r} + (\vec{a}\cdot\vec{c})\vec{b} - (\vec{a}\cdot\vec{b})\vec{r} &= \vec{c}\\
    \vec{r}(1 - (\vec{a} \cdot \vec{b})) &= \vec{c} - (\vec{a} \cdot \vec{c})\vec{b}
\end{align*}
We wish to divide, so casework must be done. If $\vec{a} \cdot \vec{b} \ne 1$, then
\[
\vec{r} = \frac{\vec{c} - (\vec{a}\cdot\vec{c})\vec{b}}{1 - \vec{a}\cdot\vec{b}}.
\]
Otherwise, $\vec{a} \cdot \vec{b} = 1$, then we have
\[
    \vec{0} = \vec{c} - (\vec{a}\cdot\vec{c})\vec{b}
\]
so if $\vec{c} - (\vec{a}\cdot\vec{c})\vec{b} \ne \vec{0}$ then there are no solutions,
and otherwise we require $\vec{a}\cdot\vec{r} = \vec{a}\cdot\vec{c}$,
which is a plane with $\vec{a}$ as a normal vector.

\subsection{Index Notation \& Summation Notation}

Consider an orthonormal right-handed basis $\{\vec{e}_1,\vec{e}_2,\vec{e}_3\}$.
We write vectors in terms of coordinates in this basis.

We let $i,j,k$ be indices with values from $1,2,3$.

\subsubsection*{Kronecker Delta}

\begin{definition}
    We define the Kronecker delta by
    \[
        \delta_{ij} = \begin{cases}
            1 & \text{if } i = j\\
            0 & \text{if } i \ne j\\
        \end{cases}
    \]
\end{definition}

Note that it is symmetric, as in $\delta_{ij} = \delta_{ji}$. Now we can write
\[
    \vec{e}_i \cdot \vec{e}_j = \delta_{ij}
\]
since $\{\vec{e}_1, \vec{e}_2, \vec{e}_3\}$ are orthonormal bases.

\subsubsection*{Levi-Civita Epsilon}
\begin{definition}
    We define the Levi-Civita epsilon by
    \[
        \varepsilon_{ijk} = \begin{cases}
            1 & \text{if } (i,j,k) \text{ is an even permutation of } (1,2,3)\\
            -1 & \text{if } (i,j,k) \text{ is an odd permutation of } (1,2,3)\\
            0 & \text{otherwise}
        \end{cases}
    \]
\end{definition}
In this case (3D), even permutations are cyclic permutations,
and odd permutations are anticyclic permutations. So we have:
\begin{align*}
    \varepsilon_{123} = \varepsilon_{231} = \varepsilon_{312} = 1\\
    \varepsilon_{321} = \varepsilon_{213} = \varepsilon_{132} = -1\\
\end{align*}
Note that it is antisymmetric between any two parameters. 
Its significance appears in a definition of the cross product:
\[
    \vec{e}_i \times \vec{e}_j = \sum_{k=1}^{3}\varepsilon _{ijk} \vec{e}_k
\]
Thus, by decomposing vectors into components along the bases, we have
\[
    \vec{a} \times \vec{b} = \sum_{i,j,k=1}^{3}\varepsilon _{ijk} a_i b_j \vec{e}_k
\]

\subsubsection*{Einstein Summation Notation}

Indices that appear exactly twice in a term are
summed over all possible values, ommitting the sum.

Examples:
\begin{compactenum}[(i)]
\item $a_i \delta_{ij} = a_j$
\item $\vec{a} \cdot \vec{b} = a_i b_i$
\item $(a \times b)_i = \varepsilon_{ijk}a_jb_k$
\item $\vec{a} \cdot (\vec{b} \times \vec{c}) = \varepsilon _{ijk}a_ib_jc_k$
\item $\delta _{ii} = \sum_{i=1}^{3}\delta_{ii} = 3$
\end{compactenum}

Rules:
\begin{compactitem}
\item If an index appears exactly once, it is a ``free index'',
    since the value of the term depends on the value of the index.
    We have that the same free indices must appear on 
    both sides of the expression.
\item If an index appears twice, then it is summed over. 
    It is a ``contracted index''.
\item No index appears more than twice in the same term.
\end{compactitem}

With this, we can now write down some identities:
\begin{compactenum}[(i)]
\item \begin{align*}
        \varepsilon _{ijk}\varepsilon _{pqr} &= \delta_{ip}(\delta_{jq}\delta_{kr} - \delta_{jr}\delta_{kq})\\
                                             &+ \delta_{iq}(\delta_{jr}\delta_{kp} - \delta_{jp}\delta_{kr})\\
                                             &+ \delta_{ir}(\delta_{jp}\delta_{kq} - \delta_{jq}\delta_{kp})\\
    \end{align*}
\item in particular, $\varepsilon_{ijk}\varepsilon_{pqk} = \delta_{ip}\delta_{jq} - \delta _{iq}\delta _{jp}$
\item in particular, $\varepsilon _{ijk}\varepsilon _{pjk} = 2\delta_{ip}$
\item in particular, $\varepsilon_{ijk}\varepsilon_{ijk} = 6$.
\end{compactenum}

Now we have the machinary to prove the following:
\[
\vec{a} \times (\vec{b} \times \vec{c}) = (\vec{a} \cdot \vec{c})\vec{b} - (\vec{a} \cdot \vec{b})\vec{c}.
\]
\begin{proof}
    \begin{align*}
        [\vec{a} \times (\vec{b}\times\vec{c})]_i 
        &= \varepsilon _{ijk}a_j(b \times c)_k\\
        &= \varepsilon _{ijk} a_j \varepsilon _{kpq} b_p c_q\\
        &= \varepsilon _{ijk}\varepsilon _{pqk} a_j b_p c_q\\
        &= (\delta _{ip}\delta _{jq} - \delta _{iq}\delta _{jp}) a_j b_p c_q\\
        &= \delta _{ip}\delta _{jq} a_j b_p c_q - \delta _{iq}\delta _{jp} a_j b_p c_q\\
        &= a_j c_j b_i - a_j b_j c_i\\
        [\vec{a} \times (\vec{b}\times\vec{c})]_i 
        &= (\vec{a}\cdot\vec{c})b_i - (\vec{a}\cdot\vec{b})c_i\\
        \vec{a} \times (\vec{b}\times\vec{c})
        &= (\vec{a}\cdot\vec{c})\vec{b} - (\vec{a}\cdot\vec{b})\vec{c}
    \end{align*}
\end{proof}

This can also be used for spherical trigonometry.

\begin{prop}
    $\vec{a},\vec{b},\vec{c} \in \mathbb{R}^{3}$, then
    \[
        (\vec{a}\times\vec{b})\cdot(\vec{b}\times\vec{c})
        = (\vec{a}\cdot\vec{b})(\vec{b}\cdot\vec{c})
        - (\vec{a}\cdot\vec{c})|\vec{b}|^{2}
    \]
\end{prop}
\begin{proof}
    \begin{align*}
        (\vec{a}\times\vec{b})_i (\vec{b}\times\vec{c})_i
        &= \varepsilon_{ijk}a_jb_k \varepsilon_{ipq}b_pc_q\\
        &= \varepsilon_{ijk}\varepsilon_{ipq} a_jb_k b_pc_q\\
        &= (\delta_{jp}\delta_{kq} - \delta_{jq}\delta_{pk}) a_jb_k b_pc_q\\
        &= \delta_{jp}\delta_{kq}a_jb_k b_pc_q - \delta_{jq}\delta_{pk} a_jb_k b_pc_q\\
        &= a_jb_kb_jc_k - a_jb_kb_kc_j\\
        &= (a_jb_j)(b_kc_k) - (a_jc_j)(b_kb_k)\\
        &= (\vec{a}\cdot\vec{b})(\vec{b}\cdot\vec{c}) - (\vec{a}\cdot\vec{c})|\vec{b}|^{2}
    \end{align*}
\end{proof}

Let a unit sphere centered at the origin,
and $A,B,C$ on the surface with position vectors $\vec{a},\vec{b},\vec{c}$.
We define distance $\delta(A,B)$ to be the length of the
shortest arc between $A$ and $B$.
We let $\alpha = \angle BAC$ formed by these
shortest length arcs.

Since the sphere is unit, $\delta(A,B) = \angle AOB$ in radians,
so we have $\vec{a}\cdot\vec{b} = \cos\delta(A,B)$.
In the same way, $|\vec{a}\times\vec{b}| = \sin\delta(A,B)$.
Now, we have
\begin{align*}
    \cos\alpha 
    &= \frac{(\vec{a}\times\vec{b})\cdot(\vec{a}\times\vec{c})}{|\vec{a}\times\vec{b}||\vec{a}\times\vec{c}|}\\
    &= \frac{(\vec{b}\cdot\vec{c})|\vec{a}|^{2} - (\vec{b}\cdot\vec{a})(\vec{a}\cdot\vec{c})}
    {|\vec{a}\times\vec{b}||\vec{a}\times\vec{c}|}\\
    \cos\alpha \sin\delta(A,B) \sin\delta(A,C)
    &= \cos\delta(B,C) - \cos\delta(B,A)\cos\delta(A,C)\\
    \cos\alpha
    &= \frac{\cos a - \cos c \cos b}{\sin c \sin b}
\end{align*}
Which is cosine rule on a sphere.

\subsubsection*{Spheres}

A sphere in $\mathbb{R}^{3}$ with center $\vec{0}$ and radius $r > 0$
is given by
\[
\Sigma = \{\vec{x} \in \mathbb{R}^{3} : |\vec{x}| = r\}.
\]
More generally, a hypersphere in $\mathbb{R}^{n}$ with center $\vec{a}$
and radius $r > 0$ is given by
\[
\Sigma = \{\vec{x} \in \mathbb{R}^{n} : |\vec{x} - \vec{a}| = r\}.
\]
\subsection[R n]{$\mathbb{R}^{n}$}
\[
\mathbb{R}^{n} = \{\vec{x} = (x_{1},x_{2},\cdots,x_n) : x_i \in \mathbb{R}\}
\]
Addition and scalar multiplication are both by component.

Things:
\begin{compactenum}[(i)]
\item The standard canonical basis is $(\vec{e}_i)_j = \delta_{ij}$.
\item The inner product is defined by $\vec{x}\cdot\vec{y} = x_iy_i$.
\item $\vec{e}_i \cdot \vec{e}_j = \delta_{ij}$
\item $x_i = \vec{e}_i \cdot x$
\item $\vec{x}\cdot\vec{y} = \delta_{ij}x_iy_j = x_iy_i$
\item We can define an additional scalar product $[\vec{a},\vec{b}] = \varepsilon_{ij}a_ib_j = a_{1}b_{2}-a_{2}b_{1}$.
    It represents the signed area of a parallelogram.
    Comparably, the scalar triple product calculate the signed volume
    of a parallelepiped.
\end{compactenum}

\subsection[C n]{$\mathbb{C}^{n}$}
\[
\mathbb{C}^{n} = \{\vec{z} = (z_{1},z_{2},\cdots,z_n) : z_j \in \mathbb{C}\}
\]
Addition and scalar multiplication are both by component.
However, depending on the choice of scalar,
the space can be different.

If we select $\mathbb{R}$ as the scalar space,
we generate $\mathbb{C}^{n}$ the real vector space.
If we select $\mathbb{C}$ as the scalar space,
then $\mathbb{C}^{n}$ is a complex vector space.

The real vector space $\mathbb{C}^{n}$ standard
canonical basis is composed of two parts:
$(\vec{e}_j)_k = \delta_{jk}$ and 
$(\vec{f}_j)_k = i\delta_{jk}$. 
The basis is formed by the union of those two sets.
Now, each $\vec{z}$ has coordinates $x_j,y_j$ such that
\[
\vec{z} = x_j \vec{e}_j + y_k \vec{f}_k.
\]
Thus, real vector space $\mathbb{C}^{n}$ is a $2n$ dimensional vector space

The complex vector space $\mathbb{C}^{n}$
no longer has $\vec{f}_j$ as part of its basis,
since $\vec{f}_j = i \vec{e}_j$,
and thus we have $\vec{z} = z_j \vec{e}_j$,
and the space is $n$ dimensional.

The inner product on $\mathbb{C}^{n}$ is defined by
\[
    (\vec{z},\vec{w}) = \conj z_j w_j
\]
Properties of the inner product:
\begin{compactenum}[(i)]
\item Hermitian: $(\vec{z},\vec{w}) = \conj{(\vec{w},\vec{z})}$.
\item Linear / Anti-linear:
    \begin{align*}
        (\vec{z},\lambda\vec{w} + \lambda'\vec{w'}) 
        &= \lambda(\vec{z},\vec{w}) + \lambda'(\vec{z},\vec{w'})\\
        (\mu\vec{z}+\mu'\vec{z'},\vec{w}) 
        &= \conj\mu(\vec{z},\vec{w}) + \conj\mu'(\vec{z'},\vec{w})
    \end{align*}
\item Positive definite: $(\vec{z},\vec{z}) = \conj z_j z_j \ge 0$.
\item $(\vec{z},\vec{z}) = 0$ iff $\vec{z} = \vec{0}$.
\end{compactenum}
Thus, the norm $\abs{\vec{z}}^{2} = (\vec{z},\vec{z})$ is defined.
Now we can also defined orthogonality in that $(\vec{z},\vec{w})=0$.
Under these definitions, the standard basis for complex
vector space $\mathbb{C}^{n}$ is orthonormal, since $(\vec{e}_j, \vec{e}_k) = \delta_{jk}$.

\subsubsection{From complex to real inner products}

For $n=1$, $z,w \in \mathbb{C}$, $(z,w) = \conj z w$.
Now, we can write $z=a_{1}+ia_{2}$ and $w=b_{1}+ib_{2}$,
and $\vec{a}=(a_{1},a_{2})$ and $\vec{b}=(b_{1},b_{2})$ are vectors in $\mathbb{R}^{2}$.
Then, $(z,w) = \conj z w = \vec{a} \cdot \vec{b} + i[\vec{a},\vec{b}]$,
where $[\vec{a},\vec{b}]$ is the alternative scalar product,
given by the Levi-Civita epsilon.

\subsection{Vector Spaces}

\begin{definition}
    Let $F$ a field, then
    a \emph{vector space over $F$} is a collection of vectors $V$
    with two operations: vector addition $V \times V \to V$ 
    and scalar multiplication $F \times V \to V$,
    satisfying the following axioms:
    \begin{compactenum}
    \item $V$ forms an abelian group under vector addition.
    \item scalar multiplication distributes over vector addition on the left and the right.
    \item $\lambda(\mu)\vec{a} = (\lambda\mu)\vec{a}$.
    \item Let $1$ be the multiplicative identity in $F$. Then, $1\vec{a} = \vec{a}$.
    \end{compactenum}
\end{definition}

A real vector space is a vector space over $\mathbb{R}$
and a complex vector space is one over $\mathbb{C}$.

\begin{definition}
    Given a real vector space $\mathcal{V}$ and 
    $\vec{v}_1,\cdots,\vec{v}_r \in V$,
    we can write a \emph{linear combination}:
    \[
    \lambda_1 \vec{v}_1 + \cdots + \lambda_r \vec{v}_r \in \mathcal{V}
    \]
    for any $\lambda_i \in \mathbb{R}$.
\end{definition}

\begin{definition}
    The \emph{span} of $\{\vec{v}_1,\cdots,\vec{v}_r\}$ is
    the set of all the possible linear combinations
    of $\{\vec{v}_1,\cdots,\vec{v}_r\}$.
\end{definition}

\begin{definition}
    A \emph{subspace} of $\mathcal{V}$ is a $V' \subseteq V$ that forms
    a vector space under the same field,
    vector addition and scalar multiplication.
\end{definition}

Equivalently, a non-empty subset of $\mathcal{U} \subseteq \mathcal{V}$
is a subspace if it satisfies that
for every $\vec{v},\vec{w} \in \mathcal{U}$, 
their linear combination is also in $\mathcal{U}$.
In particular, for any $\vec{v}_{1},\cdots,\vec{v}_r \in \mathcal{V}$,
their span is a subspace.

We call the subspaces $\{\vec{0}\}$ and $\mathcal{V}$ of a vector
space $\mathcal{V}$ the trivial subspaces.

\subsubsection{Linear Independence and Dependence}

Let $\mathcal{V}$ a real or complex vector space, and $\vec{v}_1,\cdots,\vec{v}_n \in \mathcal{V}$.
Consider a linear combination $\lambda_i \vec{v}_i$.
where $\lambda_i \in F$.

\begin{definition}
    A collection of vectors $\vec{v}_1,\cdots,\vec{v}_n \in \mathcal{V}$
    are said to be \emph{linearly independent}
    if $\lambda_i \vec{v}_i = \vec{0}$ implies that $\lambda_i = 0$ for all $i$.
    Otherwise, they are said to be \emph{linearly dependent}.
\end{definition}

Examples:
\begin{compactenum}[(i)]
\item For example, $(0,1)$ and $(0,2)$ in $\mathbb{R}^{2}$ is linearly
    dependent since $2\lambda (0,1) - \lambda (0,2) = \vec{0}$ for all $\lambda \in \mathbb{R}$.
\item The canonical basis of $\mathbb{R}^{n}$ are linearly independent.
\item Any set containing $\vec{0}$ is linearly dependent,
    since the coefficient in front of $\vec{0}$ does not affect the result.
\item In $\mathbb{R}^{3}$, $\vec{a},\vec{b},\vec{c}$ are linearly independent
    iff $\vec{a}\cdot(\vec{b}\times\vec{c}) \ne 0$.
\end{compactenum}

\subsubsection{Inner Products}

The inner product is a binary operation $V \times V \to F$.
For $\vec{v},\vec{w} \in \mathcal{V}$, we denote their
inner product by $\vec{v}\cdot\vec{w}$ or $(\vec{v},\vec{w})$.
It must satisfy the following:
\begin{compactenum}[(i)]
\item Hermitian: $(\vec{v},\vec{w}) = \conj{(\vec{w},\vec{v})}$.\\
    (Note that this becomes commutativity if $\mathcal{V}$ is a real vector space.)
\item Linear / Anti-linear:
    \begin{align*}
        (\vec{z},\lambda\vec{w} + \lambda'\vec{w'}) 
        &= \lambda(\vec{z},\vec{w}) + \lambda'(\vec{z},\vec{w'})\\
        (\mu\vec{z}+\mu'\vec{z'},\vec{w}) 
        &= \conj\mu(\vec{z},\vec{w}) + \conj\mu'(\vec{z'},\vec{w})
    \end{align*}
    (Note again that this becomes linearity on both sides if $\mathcal{V}$ is real.)
\item Positive Definite: $(\vec{v},\vec{v}) \ge 0$, and equality iff $\vec{v} = \vec{0}$.
\end{compactenum}

From here, we can define a norm $\abs{\vec{v}}^{2} = (\vec{v},\vec{v})$, and
orthogonality by $\vec{v}\cdot\vec{w} = 0$.

\begin{prop}
    If $\vec{v}_1,\cdots,\vec{v}_n$ are non-zero and orthogonal,
    then they are linearly independent.
\end{prop}
\begin{proof}
    Let $\alpha_i \vec{v}_i = \vec{0}$. Then,
    \begin{align*}
        \vec{0} = \inner{\vec{v}_j}{\alpha_i \vec{v}_i}
        &= \alpha_i\inner{\vec{v}_j}{\vec{v}_i}\\
        &= \alpha_i\inner{\vec{v}_i}{\vec{v}_i}\\
        &= \alpha_i\abs{\vec{v}_i}^{2} = \vec{0}
    \end{align*}
    Thus, since $\vec{v}_i$ is non-zero, $\alpha_i = 0$.
\end{proof}

\subsubsection{Bases and Dimensions}

Given a vector space $\mathcal{V}$, a basis is a set 
$\mathcal{B} = \{\vec{e}_1,\cdots,\vec{e}_n\}$
such that $\mathcal{B}$ spans $\mathcal{V}$, i.e. $\vecspan \mathcal{B} = \mathcal{V}$,
and $\mathcal{B}$ is linearly independent.

This implies that there is a unique way $v_i$
to decompose $\vec{v} = v_i \vec{e}_i$.
$v_i$ is called the \emph{components} of $\vec{v}$ wrt $\mathcal{B}$.

\begin{theorem}
    If $\{\vec{e}_1,\cdots,\vec{e}_n\}$ and $\{\vec{f}_i,\cdots,\vec{f}_m\}$
    are bases for $\mathcal{V}$, then $n=m$.
\end{theorem}

This means that all bases for a given vector space
have the same size, so we call this size $n$ the \emph{dimension} of the vector space.

\begin{prop}
    If $\mathcal{V}$ is a vector space of dimension $n$, then
    \begin{compactenum}[(i)]
    \item If $\mathcal{Y} = \{\vec{w}_1,\cdots,\vec{w}_m\}$ spans $\mathcal{V}$ and $m \ge n$,
        we can remove vectors from $\mathcal{Y}$ to get a basis.
    \item If $\mathcal{Z} = \{\vec{u}_1,\cdots,\vec{u}_k\}$ is linearly independent
        then $k \le n$. In particular, if $k < n$,
        vectors can be added to obtain a basis.
    \end{compactenum}
\end{prop}

\end{document}

